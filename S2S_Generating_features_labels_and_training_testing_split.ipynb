{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c7cb5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled set - (9603, 289, 1) (9603, 40, 1)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = 'data/data/Lifetime_Prediction_Benchmark16_DEC_2020_Train_features.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 302)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1e03a8875e15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;31m# call the functions to generate the arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m \u001b[0mX_Data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_Data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_dataset_from_rawfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatlab_trainingset_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_structure_variable_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_arrays_save_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[0mX_Test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_Test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_dataset_from_rawfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatlab_testingset_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_structure_variable_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_arrays_save_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-1e03a8875e15>\u001b[0m in \u001b[0;36mgenerate_dataset_from_rawfile\u001b[1;34m(datapath, dataname, savepath, shuffle)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0mfeaturespath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msavepath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'features.h5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mlabelspath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msavepath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'labels.h5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturespath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstoredata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0mstoredata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"features\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeatures_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabelspath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstoredata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\battery_env\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\battery_env\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;31m# Open in append mode (read/write).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to create file (unable to open file: name = 'data/data/Lifetime_Prediction_Benchmark16_DEC_2020_Train_features.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 302)"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "#\n",
    "# Script to generate the training and testing\n",
    "# arrays seperated into features and labels.\n",
    "# Authors: Neil Sengupta, Weihan Li\n",
    "#\n",
    "######################################################################\n",
    "# Comments:\n",
    "#\n",
    "# This script uses the 'mat4py', 'h5py', and 'sklearn' packages\n",
    "# which needs to be installed prior to usage\n",
    "#\n",
    "######################################################################\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import mat4py as mpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import h5py as st\n",
    "\n",
    "def array_shuffler(array_1, array_2):\n",
    "    # function to shuffle two arrays of equal length simultaneously\n",
    "    # while maintaining correlation between individual rows\n",
    "    assert len(array_1) == len(array_2)  # confirm equal lengths\n",
    "    # generate random permutation of rows up to array size\n",
    "    p = np.random.permutation(len(array_1))\n",
    "    # slice both arrays with the same permutation matrix\n",
    "    return array_1[p], array_2[p]\n",
    "\n",
    "def generate_dataset_from_rawfile(datapath, dataname, savepath, shuffle=True):\n",
    "    # function to take the \"test/train.mat\" files generated by the MATLAB script\n",
    "    # and split the structure into feature arrays and label arrays\n",
    "    # with an option to shuffle array rows\n",
    "\n",
    "    # datapath = The full file path of the .mat file to be used as input\n",
    "    # dataname = The struct variable name present in the .mat file\n",
    "    # savepath = The full file path of the location for saving the arrays\n",
    "    # shuffle  = If true, shuffles the feature and label array simultaneously\n",
    "\n",
    "    # temporary dictionary from .mat file\n",
    "    data_loader = mpy.loadmat(datapath)[dataname]\n",
    "    # Generate pandas data frame 'data' from the .mat file\n",
    "    data = pd.DataFrame.from_dict(data_loader)\n",
    "\n",
    "    number_of_rows = len(data)                                  # number of rows of the data frame\n",
    "    number_of_samples = 0                                       # variable for number of samples in each input row\n",
    "    number_of_targets = 0                                       # variable for number of samples in each target row\n",
    "    inputlist = []                                              # initiate input array as list\n",
    "    targetlist = []                                             # initiate target array as list\n",
    "\n",
    "    # Extract each row from the data frame\n",
    "    for row in range(number_of_rows):\n",
    "        input_row = data.iloc[row, -2]                          # extract capacity history as input\n",
    "        number_of_samples = len(input_row)                      # number of samples in the current input row\n",
    "        current_input_row = np.zeros((number_of_samples, 1))    # initiate current input row as numpy array\n",
    "        for sample in range(number_of_samples):\n",
    "            # populate current row with sample values from capacity history\n",
    "            current_input_row[sample, 0] = input_row[sample]\n",
    "        inputlist.append(current_input_row)                     # append current row to input list\n",
    "\n",
    "        target_row = data.iloc[row, -1]                         # extract future capacity degradation curve as output\n",
    "        number_of_targets = len(target_row)\n",
    "        targetlist.append(target_row)                           # append current target row to target list\n",
    "\n",
    "    # convert input and target lists to numpy arrays and assert proper dimensions\n",
    "    input_array = np.asarray(inputlist).reshape((number_of_rows, number_of_samples, 1))\n",
    "    target_array = np.asarray(targetlist).reshape((number_of_rows, number_of_targets, 1))\n",
    "\n",
    "    if shuffle:\n",
    "        # shuffle input and target arrays in tandem if shuffle = true in function parameters\n",
    "        features_file, labels_file = array_shuffler(input_array, target_array)\n",
    "        print('Shuffled set', end=' - ')\n",
    "    else:\n",
    "        # If shuffle = false, then just pass the original arrays\n",
    "        print('Non Shuffled set', end=' - ')\n",
    "        features_file, labels_file = input_array, target_array\n",
    "\n",
    "    print(features_file.shape, labels_file.shape)             # confirm proper dimensions of the input and target files\n",
    "\n",
    "    # save the features and labels in appropriate locations\n",
    "    featurespath = savepath + 'features.h5'\n",
    "    labelspath = savepath + 'labels.h5'\n",
    "    with st.File(featurespath, 'w') as storedata:\n",
    "        storedata.create_dataset(\"features\", data=features_file)\n",
    "    with st.File(labelspath, 'w') as storedata:\n",
    "        storedata.create_dataset(\"labels\", data=labels_file)\n",
    "\n",
    "    return features_file, labels_file\n",
    "\n",
    "\n",
    "filepath_of_datasets = 'data/'                            # Enter full file path of the .mat files of the dataset\n",
    "file_matlab_nametag = 'S2SLearning_25-Aug-2021_'       # Enter the version tag of the file generated in MATLAB\n",
    "output_nametag = 'Lifetime_Prediction_Benchmark'             # Enter the output tag of the files to be saved\n",
    "output_datetag = '16_DEC_2020'                               # Enter the date as part of the file name to be saved\n",
    "training_structure_variable_name = 'Train_Set'               # Variable name of the training set in the .mat structure\n",
    "testing_structure_variable_name = 'Test_Set'                 # Variable name of the testing set in the .mat structure\n",
    "\n",
    "# generate the full file path and names of the .mat files to be used as input to the functions\n",
    "matlab_trainingset_filepath = filepath_of_datasets + file_matlab_nametag + training_structure_variable_name + '.mat'\n",
    "matlab_testingset_filepath = filepath_of_datasets + file_matlab_nametag + testing_structure_variable_name + '.mat'\n",
    "\n",
    "# generate the full file path for saving the features and labels arrays from the functions\n",
    "training_arrays_save_path = filepath_of_datasets + output_nametag + output_datetag + '_Train_'\n",
    "testing_arrays_save_path = filepath_of_datasets + output_nametag + output_datetag + '_Test_'\n",
    "\n",
    "# call the functions to generate the arrays\n",
    "X_Data, y_Data = generate_dataset_from_rawfile(matlab_trainingset_filepath, training_structure_variable_name, training_arrays_save_path, shuffle=True)\n",
    "X_Test, y_Test = generate_dataset_from_rawfile(matlab_testingset_filepath, testing_structure_variable_name, testing_arrays_save_path, shuffle=False)\n",
    "\n",
    "# split the training array further for training and validation while training each epoch\n",
    "# This part can be ignored if training-validation split is done directly inside the model\n",
    "training_validation_fraction = 0.15            # fraction of samples to be used as validation while training\n",
    "# The above variable is a percentage to fraction, and thus must be between 0 to 1\n",
    "\n",
    "X_train, X_val, y_train, y_val = tts(X_Data, y_Data, test_size=training_validation_fraction, shuffle=True)\n",
    "print('--- shuffled split as: ', X_train.shape[0], ' samples, and testing: ', X_val.shape[0], 'samples ---')\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Further code for model creation and training\n",
    "######################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ce8a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
