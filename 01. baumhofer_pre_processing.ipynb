{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c46686f",
   "metadata": {},
   "source": [
    "## Baumhofer 2014 - Data Pre-processing\n",
    "This notebook contains pre-processing code for Baumhofer 2014 data.<br>\n",
    "Some of the steps take a couple of minutes to run.<br>\n",
    "\n",
    "### Note:\n",
    "- Change the filepaths to be correct for your system. \n",
    "\n",
    "The overall procedure is as follows. For a particular cell:<br>\n",
    "- Identify the relevant CSV files containing the cycle data<br>\n",
    "- Load data from CSV, selecting only a subset of columns<br>\n",
    "- Use cycle numbers obtained from cycling CSV files to build an interpolated capacity curve<br>\n",
    "- Consider only a subset of the time series data by isolating the charge state and the voltage range stated in the paper<br>\n",
    "- Convert datetime string array to float array of \"seconds elapsed since cycle start\" for every cycle<br>\n",
    "- Remove duplicate entries containing identical time/voltage values present at the start of each cycle<br>\n",
    "- Impose a threshold for the lower limit of the number of samples present in the time series array for any cycle. Generate a list of cycles that should be kept<br>\n",
    "- Store the time series data (X) and capacity array (y), with the threshold applied<br>\n",
    "\n",
    "There is one dictionary that stores all of the information for every cell.<br>\n",
    "Then, on the dataset level:<br>\n",
    "- Find the length of the largest array of time series data (with threshold condition(s) applied)<br>\n",
    "- Pad all time series arrays that are shorter than this maximum length, using np.nan values<br>\n",
    "- Split into train, validation and test sets, on the cell level<br>\n",
    "- Scale the X values per-feature using min max scaling. Apply scaling from training to validation and test sets.<br>\n",
    "- Replace remaining np.nan values with zeros<br>\n",
    "- Re-shape the data so it is suitable for an LSTM<br>\n",
    "\n",
    "## Basic LSTM implementation\n",
    "A basic implementation of the LSTM described in the paper can be found here:<br>\n",
    "https://colab.research.google.com/drive/1CMdNmfOVhmEebm4StM_Gr13Kv4PJad7f?usp=sharing<br>\n",
    "It takes the voltage and time arrays as input and predicts the remaining capacity at that particular cycle.\n",
    "\n",
    "<b>NOTE:</b> To use this, you will need to save the X and y data to Google Drive and mount your Google Drive within Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5275acd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from baumhofer_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a6ef9e",
   "metadata": {},
   "source": [
    "### Load Capacity Data From File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f0c7838",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/baumhofer_2014_capacity.pkl', 'rb') as a_file:\n",
    "    baumhofer_cap = pickle.load(a_file)\n",
    "\n",
    "# Change the keys in baumhofer_cap to match the cell IDs in the CSV folder/file names e.g. 002, 003, ...\n",
    "csv_cell_numbers = [str(val).zfill(3) for val in range(2, 50)]\n",
    "\n",
    "# Use a dictionary comprehension to get the keys from the csv_cell_numbers list\n",
    "# and get the corresponding cycle/capacity arrays from the baumhofer_cap dictionary\n",
    "baumhofer_cap = {k: baumhofer_cap[str(i)] for i, k in enumerate(csv_cell_numbers)}\n",
    "\n",
    "# Remove the rows with closely spaced cycle numbers in the Baumhofer capacity data\n",
    "baumhofer_cap = remove_close_cycles(baumhofer_cap, to_plot=False, verbose=False)\n",
    "\n",
    "# Delete intermediate variables to keep variable explorer clean\n",
    "del a_file, csv_cell_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f9e41c",
   "metadata": {},
   "source": [
    "### Specify Cycling CSV File Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "481a290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"E:/new_german_data_full/extracted/\"\n",
    "assert(os.path.exists(file_dir))\n",
    "files = glob.glob(file_dir + \"**/*.csv\", recursive=True)\n",
    "files = np.array(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7685a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove problematic or unnecessary files from the files array\n",
    "bad_fnames = ['E:/new_german_data_full/extracted\\e production=ep sanyo 019=ZYK=Massenalterung=2013-01-21 094031=TBA_Zyk=TS011120  Format01=Kreis 5-082\\e production=ep sanyo 019=ZYK=Massenalterung=2013-01-21 094031=TBA_Zyk=TS011120  Format01=Kreis 5-082.csv',\n",
    "              'E:/new_german_data_full/extracted\\e production=ep sanyo 030=ZYK=Massenalterung=2013-04-22 055352=TBA_Zyk=TS015979  Format01=Kreis 5-089\\e production=ep sanyo 030=ZYK=Massenalterung=2013-04-22 055352=TBA_Zyk=TS015979  Format01=Kreis 5-089.csv']\n",
    "\n",
    "for fname in bad_fnames:\n",
    "    problem_index = np.array([i for i, filename in enumerate(files) if fname in filename])\n",
    "    for idx in problem_index:\n",
    "        files = np.delete(files, idx, axis=0)\n",
    "        \n",
    "# There are folders with \"hundekuchen\" in the name. These are garbage. Remove them\n",
    "hundekuchen_indices = [i for i, filename in enumerate(files) if \"hundekuchen\" in filename]\n",
    "files = np.delete(files, hundekuchen_indices, axis=0)\n",
    "\n",
    "del bad_fnames, problem_index, hundekuchen_indices, idx, file_dir, fname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b9b252",
   "metadata": {},
   "source": [
    "### German to English Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91685741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time\n"
     ]
    }
   ],
   "source": [
    "# Set up the translation into English for the column headers\n",
    "# German column headers\n",
    "german = [\"Schritt\",\"Zustand\",\"Zeit\",\"Programmdauer\",\"Schrittdauer\",\"Zyklus\",\n",
    "          \"Zyklusebene\",\"Prozedur\",\"Prozedurebene\",\"AhAkku\",\"AhLad\",\"AhEla\",\n",
    "          \"AhStep\",\"Energie\",\"WhStep\",\"Spannung\",\"Strom\",\"Temp13\"]\n",
    "\n",
    "# English translations\n",
    "english = [\"step\", \"state\", \"time\", \"programme duration\", \"step duration\", \"cycle\",\n",
    "           \"cycle level\", \"procedure\", \"procedure level\", \"Qacc\", \"Qcha\", \"Qdch\",\n",
    "           \"AhStep\", \"energy\", \"WhStep\", \"voltage\", \"current\", \"temp13\"]\n",
    "\n",
    "# Check list lengths match\n",
    "assert(len(german) == len(english))\n",
    "\n",
    "# Create a dictionary and view a test entry\n",
    "translate = dict(zip(german, english))\n",
    "print(translate['Zeit'])\n",
    "\n",
    "# Specify a converter dictionary for use with pd.read_csv, to specify data types\n",
    "# of columns contained within the CSV. Use the original German column names.\n",
    "# Leave \"time\" column data type as \"object\" for now. Handle using datetime later.\n",
    "\n",
    "dtypes = [int, str, object, float, float, int, int, str, int,\n",
    "          float, float, float, float, float, float, float, float, float]\n",
    "\n",
    "converter = dict(zip(german, dtypes))\n",
    "\n",
    "# Get a list of the German column names to be loaded by applying the eng2ger function\n",
    "cols = ['step', 'state', 'cycle', 'time', 'voltage']\n",
    "fields = [eng2ger(val) for val in cols]\n",
    "\n",
    "del german, english, dtypes, cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a0832",
   "metadata": {},
   "source": [
    "# Interpolate Capacity Data\n",
    "Use Piecewise Cubic Hermite Interpolating Polynomial (PCHIP) to get a capacity value for every integer cycle number over the cell's available data.<br>\n",
    "Use the cycle numbers / capacity values stored in \"baumhofer_cap\" dictionary<br>\n",
    "\n",
    "NOTE: Skip to the next section to load the result from file, to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c763d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 049...\n",
      "Finished.\n",
      "Elapsed time: 110.13477349281311\n"
     ]
    }
   ],
   "source": [
    "# Call the function to get the two capacity dictionaries\n",
    "y_cap_dict, y_cap_interp_dict = get_capacity_dictionaries(parent_dict=baumhofer_cap,\n",
    "                                                          files=files,\n",
    "                                                          fields=fields,\n",
    "                                                          converter=converter,\n",
    "                                                          translate=translate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6311de",
   "metadata": {},
   "source": [
    "## Load Interpolated Capacity From File\n",
    "Rather than running the code above, load the data from saved pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62cdb439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-interpolated capacity dictionary\n",
    "with open(\"./processed_data/capacity_dict.pkl\", \"rb\") as a_file:\n",
    "    y_cap_dict = pickle.load(a_file)\n",
    "\n",
    "# Interpolated capacity dictionary\n",
    "with open(\"./processed_data/capacity_dict_interp.pkl\", \"rb\") as a_file:\n",
    "    y_cap_interp_dict = pickle.load(a_file)\n",
    "    \n",
    "del a_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f77a09",
   "metadata": {},
   "source": [
    "# Load Cycle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7197d366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_func(cell_filepaths, idx_list):\n",
    "    '''\n",
    "    NEEDS A BETTER NAME\n",
    "    Description\n",
    "    -----------\n",
    "    \n",
    "    This function should be used to load the cycling data from\n",
    "    CSV files in between characterisation tests. Specifically,\n",
    "    one or more consecutive cycling data files.\n",
    "    \n",
    "    This would be called multiple times - one time for each \"block\"\n",
    "    of cycling CSV files, as the length of the output of get_file_index_list().\n",
    "    \n",
    "    The master DataFrame for a cell can be built up in a\n",
    "    loop outside of this function. The point is to give this function\n",
    "    as little responsibility as possible for ease of use and\n",
    "    to make it easier to read, understand and debug.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Load the data from the first file, whose index is specified by the first entry in idx_list\n",
    "    df, _ = load_from_csv_EDIT(cell_filepaths[idx_list[0]], fields=fields, converter=converter, translate=translate)\n",
    "    # Find the maximum value of the cycle column - we need to maintain this value for later use\n",
    "    max_cycle_num = np.max(df.cycle)\n",
    "    # Debugging print statement\n",
    "    #print(max_cycle_num)\n",
    "        \n",
    "    # If there is more than one cycling CSV file, build up a DataFrame by concatenating\n",
    "    if len(idx_list) > 1:\n",
    "        # Loop over the remaining file indices in the list\n",
    "        for idx in idx_list[1:]:\n",
    "            temp_df, _ = load_from_csv_EDIT(cell_filepaths[idx], fields=fields, converter=converter, translate=translate)\n",
    "                \n",
    "            # Add the max_cycle_num to the values in the cycle column of temp_df\n",
    "            # We don't need to add 1, because the cycle numbers start at 1\n",
    "            temp_df.cycle += max_cycle_num\n",
    "            # Get an updated value for the maximum cycle number\n",
    "            max_cycle_num = np.max(temp_df.cycle)\n",
    "            # Debugging print statement\n",
    "            #print(max_cycle_num)\n",
    "            \n",
    "            # Append the temporary DataFrame onto the previous one\n",
    "            df = pd.concat((df, temp_df))\n",
    "            \n",
    "    return df, max_cycle_num\n",
    "\n",
    "\n",
    "def create_cell_dataframe_EDIT(cell_ID, cycle_file_indices, cell_filepaths, state=None, verbose=False):\n",
    "    '''\n",
    "    Description\n",
    "    -----------\n",
    "    For a specified cell, load the time series data from the CSV files into a DataFrame.\n",
    "    A state (\"CHA\" or \"DCH\") may be specified to load only charge or discharge parts of the cycles.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------   \n",
    "    files (type: list)\n",
    "        A list of all of the CSV files, for all cells.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    '''\n",
    "    \n",
    "  \n",
    "\n",
    "    print(f\"Processing cell {cell_ID}...\")\n",
    "    for i, idx_list in enumerate(cycle_file_indices):\n",
    "        if i == 0:\n",
    "            if verbose:\n",
    "                print(f\"Processing first file(s) at index: {idx_list}\")\n",
    "            # Call the function for the first time in the loop - get the \"base\" DataFrame\n",
    "            df, prev_max_cycle_num = a_func(cell_filepaths, idx_list)\n",
    "            \n",
    "            # If a specific state is specified, keep this one and discard the other (DCH vs CHA)\n",
    "            if state != None and state in df.state.unique():\n",
    "                df = df[df.state == state]\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Maximum cycle number: {prev_max_cycle_num}\")\n",
    "\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"Processing file(s) at index: {idx_list}\")\n",
    "            # Call the function for all remaining \"blocks\" of cycling files\n",
    "            next_df, max_cycle_num = a_func(cell_filepaths, idx_list)\n",
    "\n",
    "            if state != None and state in next_df.state.unique():\n",
    "                next_df = next_df[next_df.state == state]\n",
    "            \n",
    "            # Add the maximum cycle number from the last DataFrame to the cycle column\n",
    "            next_df.cycle += prev_max_cycle_num\n",
    "            # Update the value of prev_max_cycle_num to be used in the next iteration\n",
    "            prev_max_cycle_num = max_cycle_num + prev_max_cycle_num\n",
    "\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Maximum cycle number: {np.max(next_df.cycle)}\")\n",
    "            # Concatenate them\n",
    "            df = pd.concat((df, next_df))\n",
    "            \n",
    "    # Remove any NAN values\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Find out if there are missing cycle numbers\n",
    "    unique_nums = np.array(df.cycle.unique())\n",
    "    all_nums = np.arange(1, np.max(df.cycle)+1)\n",
    "    \n",
    "    # Check if there are missing cycle numbers\n",
    "    if len(np.setdiff1d(all_nums, unique_nums)) > 0:\n",
    "        missing_cycles_bool = True\n",
    "    else:\n",
    "        missing_cycles_bool = False\n",
    "    \n",
    "    if verbose:\n",
    "        if missing_cycles_bool:\n",
    "            print()\n",
    "            print(\"------------------------------------------\")\n",
    "            print(\"Check for missing/skipped cycle numbers\")\n",
    "            print(f\"There are {len(df.cycle.unique())} unique cycle numbers\")\n",
    "            print(f\"The maximum cycle number is {np.max(df.cycle)}\")\n",
    "            print(\"The missing cycle numbers are:\")\n",
    "            print(np.setdiff1d(all_nums, unique_nums))\n",
    "        else:\n",
    "            print(\"No missing cycles were found.\")\n",
    "            \n",
    "        print()\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def create_cell_dataframe(cell_ID, files, state=None, verbose=False):\n",
    "#     '''\n",
    "#     Description\n",
    "#     -----------\n",
    "    \n",
    "    \n",
    "    \n",
    "#     Parameters\n",
    "#     ----------   \n",
    "#     files (type: list)\n",
    "#         A list of all of the CSV files, for all cells.\n",
    "    \n",
    "    \n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "    \n",
    "    \n",
    "    \n",
    "#     '''\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Get the indices of the cycle CSV files for the particular cell_ID, from the list of all CSV files\n",
    "#     cycle_file_indices, cell_filepaths = get_file_index_list(cell_ID, files)\n",
    "\n",
    "#     print(f\"Processing cell {cell_ID}...\")\n",
    "#     for i, idx_list in enumerate(cycle_file_indices):\n",
    "#         if i == 0:\n",
    "#             if verbose:\n",
    "#                 print(f\"Processing first file(s) at index: {idx_list}\")\n",
    "#             # Call the function for the first time in the loop - get the \"base\" DataFrame\n",
    "#             df, prev_max_cycle_num = a_func(cell_filepaths, idx_list)\n",
    "            \n",
    "#             # If a specific state is specified, keep this one and discard the other (DCH vs CHA)\n",
    "#             if state != None and state in df.state.unique():\n",
    "#                 df = df[df.state == state]\n",
    "            \n",
    "#             if verbose:\n",
    "#                 print(f\"Maximum cycle number: {prev_max_cycle_num}\")\n",
    "\n",
    "#         else:\n",
    "#             if verbose:\n",
    "#                 print(f\"Processing file(s) at index: {idx_list}\")\n",
    "#             # Call the function for all remaining \"blocks\" of cycling files\n",
    "#             next_df, max_cycle_num = a_func(cell_filepaths, idx_list)\n",
    "\n",
    "#             if state != None and state in next_df.state.unique():\n",
    "#                 next_df = next_df[next_df.state == state]\n",
    "            \n",
    "#             # Add the maximum cycle number from the last DataFrame to the cycle column\n",
    "#             next_df.cycle += prev_max_cycle_num\n",
    "#             # Update the value of prev_max_cycle_num to be used in the next iteration\n",
    "#             prev_max_cycle_num = max_cycle_num + prev_max_cycle_num\n",
    "\n",
    "\n",
    "#             if verbose:\n",
    "#                 print(f\"Maximum cycle number: {np.max(next_df.cycle)}\")\n",
    "#             # Concatenate them\n",
    "#             df = pd.concat((df, next_df))\n",
    "            \n",
    "#     # Find out if there are missing cycle numbers\n",
    "#     unique_nums = np.array(df.cycle.unique())\n",
    "#     all_nums = np.arange(1, np.max(df.cycle)+1)\n",
    "    \n",
    "#     # Check if there are missing cycle numbers\n",
    "#     if len(np.setdiff1d(all_nums, unique_nums)) > 0:\n",
    "#         missing_cycles_bool = True\n",
    "#     else:\n",
    "#         missing_cycles_bool = False\n",
    "    \n",
    "#     if verbose:\n",
    "#         if missing_cycles_bool:\n",
    "#             print()\n",
    "#             print(\"------------------------------------------\")\n",
    "#             print(\"Check for missing/skipped cycle numbers\")\n",
    "#             print(f\"There are {len(df.cycle.unique())} unique cycle numbers\")\n",
    "#             print(f\"The maximum cycle number is {np.max(df.cycle)}\")\n",
    "#             print(\"The missing cycle numbers are:\")\n",
    "#             print(np.setdiff1d(all_nums, unique_nums))\n",
    "#         else:\n",
    "#             print(\"No missing cycles were found.\")\n",
    "            \n",
    "#         print()\n",
    "            \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "759ac837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cell 049...\n",
      "Finished\n",
      "Elapsed time: 144.8s\n"
     ]
    }
   ],
   "source": [
    "cell_IDs = list(baumhofer_cap.keys())\n",
    "#cell_IDs = ['002', '003', '004', '005', '006']\n",
    "\n",
    "# Create a dictionary to store the data for each cell.\n",
    "# Initialise it with fields for the DataFrame and the cycle numbers for which there exists time series data \n",
    "result_dict = {cell_ID: {'df': None, 'ts_cycles': None} for cell_ID in cell_IDs}\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for cell_ID in cell_IDs:\n",
    "    # IPython - clear printed output between iterations\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Get the cycle file indices and cell filepaths for the cell in focus\n",
    "    cycle_file_indices, cell_filepaths = get_file_index_list(cell_ID, files)\n",
    "    # Create the DataFrame for the cell\n",
    "    df = create_cell_dataframe_EDIT(cell_ID,\n",
    "                                    cycle_file_indices,\n",
    "                                    cell_filepaths,\n",
    "                                    state=\"CHA\",\n",
    "                                    verbose=False)\n",
    "    \n",
    "    \n",
    "    # Reset the index of the DataFrame\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Populate the result_dict with the DataFrame and array of cycle numbers with time series data present\n",
    "    result_dict[cell_ID]['df'] = df\n",
    "    \n",
    "    # Create a DataFrame that has the voltage range imposed\n",
    "    df_subset = df[(df['voltage'] >= 3.65) & (df['voltage'] <= 3.89)]\n",
    "    df_subset.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # The assignment is done here because we need to reset the indices in the df_subset DataFrame\n",
    "    result_dict[cell_ID]['df_subset'] = df_subset\n",
    "    \n",
    "    # Take the ts_cycles array based on the subset of the DataFrame with state and voltage limitations imposed\n",
    "    result_dict[cell_ID]['ts_cycles'] = np.array(df_subset.cycle.unique())\n",
    "    \n",
    "    \n",
    "end = time.time()\n",
    "print(\"Finished\")\n",
    "print(f\"Elapsed time: {end-start:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "011e2614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the result_dict to file to save time in the future\n",
    "# with open(\"./result_dict.pkl\", \"wb\") as a_file:\n",
    "#     pickle.dump(result_dict, a_file)\n",
    "# del a_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f437f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load result_dict from file\n",
    "# with open(\"./result_dict.pkl\", \"rb\") as a_file:\n",
    "#     result_dict = pickle.load(a_file)\n",
    "# del a_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d4c1789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete some variables to keep the variable explorer clean\n",
    "del start, end, cell_ID, cell_IDs, df, df_subset, cell_filepaths, cycle_file_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296e43b5",
   "metadata": {},
   "source": [
    "## Populate Dictionary with Time Series and Capacity Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eecb5f8",
   "metadata": {},
   "source": [
    "### Obtain Arrays of Time Elapsed For Each Cycle\n",
    "The time information is provided as a datetime string. We need to convert the data and manipulate it to obtain an array that contains the number of seconds elapsed since the start of a cycle, for each cycle. The functions defined below handle this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab282001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_bad_time_string(x):\n",
    "    '''\n",
    "    The datetime format specified is not satisfied when the value after\n",
    "    the decimal point in the seconds is \"0\".\n",
    "    \n",
    "    For example, we get a ValueError saying the time data:\n",
    "    2013-06-10 14:21:33.0\n",
    "    does not match the format '%Y-%m-%d %H:%M:%S.%f'\n",
    "    \n",
    "    If the value after the decimal point is changed to \"00\" instead of \"0\",\n",
    "    it works.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Isolate the string value after the decimal point\n",
    "    new_str = x.split(\".\")\n",
    "\n",
    "    if new_str[-1] == \"0\":\n",
    "        new_str[-1] = \"00\"\n",
    "        fixed_str = \".\".join(new_str[:])\n",
    "    else:\n",
    "        fixed_str = x\n",
    "        \n",
    "    return fixed_str\n",
    "\n",
    "\n",
    "# These two small functions are to be used with np.vectorize()\n",
    "def datestr_to_datetime(x):\n",
    "    '''\n",
    "    Convert a string to a datetime object.\n",
    "    The format is hard-coded to match the format of the Baumhofer data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x (type: str)\n",
    "        A string containing date and time information\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A datetime object\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        return datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    except ValueError:\n",
    "        return datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "def timedelta_to_float(x):\n",
    "    '''\n",
    "    Convert timedelta object to float.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    return x.total_seconds()\n",
    "\n",
    "\n",
    "# Vectorise the functions\n",
    "convert_time = np.vectorize(datestr_to_datetime)\n",
    "get_seconds = np.vectorize(timedelta_to_float)\n",
    "\n",
    "\n",
    "def get_elapsed_cycle_time(arr):\n",
    "    '''\n",
    "    Given an input array of datetime strings, re-express the data as an array\n",
    "    containing the number of seconds elapsed since the first time stamp in the\n",
    "    array.\n",
    "    \n",
    "    This is the function that should be called. The other related functions are\n",
    "    utility functions used within this one.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arr (type: numpy array)\n",
    "        Array of datetime strings. For this application, the format is\n",
    "        hard-coded inside the datestr_to_datetime() function. For reference,\n",
    "        the format is \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "      \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    seconds_array (type: numpy array)\n",
    "        An array of float values representing the time in seconds that has elapsed\n",
    "        since time time at the beginning of the array.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # A call to a function that will fix the instances of zero hundredths of a second\n",
    "    # not matching the expected format. Applied to every element of arr\n",
    "    arr = np.array(list(map(check_for_bad_time_string, arr)))\n",
    "    \n",
    "    # Convert the entries to pandas Timestamp object\n",
    "    datetimes = pd.to_datetime(arr)\n",
    "    \n",
    "    # Convert to Python datetime object to use the total_seconds() method\n",
    "    py_datetimes = convert_time(datetimes.astype(str))\n",
    "    \n",
    "    # Get the array of times elapsed since the first row's time stamp\n",
    "    elapsed = py_datetimes - py_datetimes[0]   # Type is timedelta\n",
    "    # Use the vectorised function to convert all the timedelta objects to floats\n",
    "    seconds_array = get_seconds(elapsed)\n",
    "    \n",
    "    return seconds_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b19cbe",
   "metadata": {},
   "source": [
    "<b>Next</b>: For every cycle, get the time series data and an array of seconds elapsed since the start of the cycle.<br>\n",
    "This takes a while to run. Later, the result is saved to file so we can save time later by just loading the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dd1e8e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [04:37<00:00,  5.78s/it]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This may be a little hard to read, but here's what's happening:\n",
    "\n",
    "* For each cell in the dictionary:\n",
    "    - Populate that cell's key \"ts_data\" with a dictionary.\n",
    "      (The dictionary's keys are the cycles for that particular cell)\n",
    "    - For each cycle:\n",
    "            + Get the time series data from \"df_subset\" (where any state and/or voltage conditions are imposed).\n",
    "            + The key \"t_elapsed\" contains an array of values that represent the number of seconds that have passed\n",
    "              since the beginning of the cycle. This array is obtained by calling get_elapsed_cycle_time().\n",
    "    \n",
    "    - Populate the cell \"capacity\" key with the dictionary that contains the interpolated capacity data.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "for cell in tqdm(result_dict):\n",
    "    result_dict[cell]['ts_data'] = {cycle: {'V': result_dict[cell]['df_subset'][result_dict[cell]['df_subset'].cycle==cycle]['voltage'].to_numpy(copy=True),\n",
    "                                            't': result_dict[cell]['df_subset'][result_dict[cell]['df_subset'].cycle==cycle]['time'].to_numpy(copy=True),\n",
    "                                            't_elapsed': get_elapsed_cycle_time(result_dict[cell]['df_subset'][result_dict[cell]['df_subset'].cycle==cycle]['time'].to_numpy(copy=True))}\n",
    "                                    for cycle in result_dict[cell]['ts_cycles']}\n",
    "\n",
    "\n",
    "    result_dict[cell]['capacity'] = copy.deepcopy(y_cap_interp_dict[cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7856d177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the result_dict to file to save time in the future.\n",
    "# # The file size is approximately 1.8 GB\n",
    "# with open(\"./result_dict.pkl\", \"wb\") as a_file:\n",
    "#     pickle.dump(result_dict, a_file)\n",
    "# del a_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a446dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load result_dict from file\n",
    "# with open(\"./result_dict.pkl\", \"rb\") as a_file:\n",
    "#     result_dict = pickle.load(a_file)\n",
    "# del a_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9997d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at one cell from result_dict to examine in the variable explorer\n",
    "a_cell = result_dict['013']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52075f6b",
   "metadata": {},
   "source": [
    "### Check for Empty Time Series Arrays or Duplicate Time Series Array Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0710dbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked 48 cells, 114523 cycles.\n",
      "No empty time series arrays found\n"
     ]
    }
   ],
   "source": [
    "empty = find_empty_time_series_arrays(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fad8752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: 109368\n",
      "Number of duplicates: 539\n",
      "Number of duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# There are duplicate values at the beginning of each time series array\n",
    "# Find out how many cycles this is the case for and remove the duplicate values\n",
    "\n",
    "def remove_initial_duplicates(result_dict):\n",
    "    # Initialise a counter for the number of duplicate datetime values\n",
    "    duplicates = 0\n",
    "    \n",
    "    # Check every cell and cycle for duplicates at the beginning of the time series arrays\n",
    "    for cell in result_dict:\n",
    "        for cycle in result_dict[cell]['ts_data']:\n",
    "            if len(result_dict[cell]['ts_data'][cycle]['V']) == 1:\n",
    "                continue\n",
    "            # If a duplicate is detected, increment duplicates counter and remove the first value in time series arrays\n",
    "            if result_dict[cell]['ts_data'][cycle]['t'][1] == result_dict[cell]['ts_data'][cycle]['t'][0]:\n",
    "                duplicates += 1\n",
    "                for var in result_dict[cell]['ts_data'][cycle].keys():\n",
    "                    result_dict[cell]['ts_data'][cycle][var] = result_dict[cell]['ts_data'][cycle][var][1:]\n",
    "                    \n",
    "    return duplicates\n",
    "         \n",
    "# Call the function initially to check for duplicates    \n",
    "duplicates = remove_initial_duplicates(result_dict)\n",
    "print(f\"Number of duplicates: {duplicates}\")\n",
    "\n",
    "# If duplicates != 0 then we enter this while loop\n",
    "while duplicates:\n",
    "    # The dictionary is modified in-place, so once no duplicates remain, the loop exits\n",
    "    duplicates = remove_initial_duplicates(result_dict)\n",
    "    print(f\"Number of duplicates: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5774d406",
   "metadata": {},
   "source": [
    "## Apply a Lower Limit to the Time Series Array Length\n",
    "Some of the cycles have time series arrays that contain only 1 or 2 values. These are useless for us.<br>\n",
    "We can define a lower limit to the time series array length, maintaining only those cycles whose time series arrays exceed this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c993a5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 049\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# The function being called is defined in baumhofer_utils.py\n",
    "\n",
    "# Debugging - Get a new dictionary, for one cell, that contains the thresholded data.\n",
    "#thresh_dict = apply_array_length_threshold(result_dict['030'], threshold_val=20, verbose=True)\n",
    "\n",
    "threshold_dict = dict.fromkeys(result_dict.keys())\n",
    "for cell in threshold_dict:\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Cell {cell}\")\n",
    "    threshold_dict[cell] = apply_array_length_threshold(result_dict[cell], threshold_val=20, verbose=False)\n",
    "    \n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c03c7",
   "metadata": {},
   "source": [
    "## Pad Every Time Series Array to the Length of the Longest Time Series Array\n",
    "Because TensorFlow wants the instances within a batch to have the same shape, we will pad all of the time series arrays so that they have the same length, equal to the maximum time series array length in the dictionary.<br>\n",
    "Notice that with the LSTM (and probably others too), we can use methods to ignore these padded values.<br>\n",
    "Specifically for LSTMs, when making a prediction, the input sequence length can be anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "923edd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_dict = pad_time_series_data(threshold_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30833714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging, take a look at one of the cells in padded_dict\n",
    "a_cell = padded_dict['020']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b3f6a",
   "metadata": {},
   "source": [
    "## Construct a 3D Array of Time Series Data\n",
    "Shape: [num_features, num_instances, num_samples]<br>\n",
    "Function is defined in baumhofer_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "afd31bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is the 3D array of time series data\n",
    "# index is an array that tells us the cell ID and cycle number for every cycle e.g. \"002_1394\"\n",
    "X, index = construct_3d_x_array(padded_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ce80ae",
   "metadata": {},
   "source": [
    "## Removing Problematic Data\n",
    "Computing the minimum and maximum values in the voltage and time data highlighted some values that we should remove.<br>\n",
    "These values were unexpected values for elapsed time e.g. 3700 seconds or -3500 seconds.<br>\n",
    "\n",
    "Before we do the scaling, let's remove the cycles that contain these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "afd8814b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000e+00 1.00000e-02 6.90000e-01 2.13000e+00 3.36000e+00 3.97000e+00\n",
      " 6.25000e+00 9.08000e+00 1.23100e+01 1.33700e+01 1.59800e+01 1.97900e+01\n",
      " 2.33300e+01 2.43900e+01 2.96400e+01 3.33500e+01 3.53600e+01 4.23000e+01\n",
      " 4.33300e+01 5.02200e+01 5.33300e+01 6.07100e+01 6.33400e+01 7.30200e+01\n",
      " 7.33600e+01 3.68337e+03 3.68876e+03 3.69333e+03 3.70337e+03 3.70636e+03]\n",
      "[ 0.00000e+00  1.00000e-02  6.00000e-02  6.00000e-02  6.10000e-01\n",
      "  1.89000e+00  3.27000e+00  3.60000e+00  6.10000e+00  8.71000e+00\n",
      "  1.17200e+01  1.32700e+01  1.55600e+01  1.93600e+01  2.32900e+01\n",
      "  2.40300e+01  2.92000e+01  3.32800e+01  3.47700e+01  4.18400e+01\n",
      "  4.32400e+01  4.97400e+01  5.32500e+01  5.97000e+01  6.32700e+01\n",
      "  7.14500e+01  7.32800e+01 -3.51672e+03 -3.51337e+03 -3.50674e+03\n",
      " -3.49671e+03 -3.49433e+03]\n"
     ]
    }
   ],
   "source": [
    "# For each feature (X.shape[0]), we have cycles (X.shape[1]) and samples (X.shape[2]).\n",
    "# For min max scaling, we could identify, on a per-feature basis, the minimum and maximum values.\n",
    "# Then, compute the scaled values, per-feature, as scaled = (val - min_val) / (max_val - min_val)\n",
    "\n",
    "# np.nanmax, np.nanmin etc are numpy functions that ignore nan values\n",
    "max_vals = np.array([np.nanmax(X[i, :, :]) for i in range(X.shape[0])])\n",
    "min_vals = np.array([np.nanmin(X[i, :, :]) for i in range(X.shape[0])])\n",
    "    \n",
    "# Examining these results highlighted some unexpected t_elapsed values e.g. around 3700 and -3500\n",
    "# These come from cell 035 and are found in keys 1078 and 1079\n",
    "print(padded_dict['035']['ts_data'][1078]['t_elapsed'])\n",
    "print(padded_dict['035']['ts_data'][1079]['t_elapsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8c1928cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of unnecessary stuff in padded_dict\n",
    "# Ignore KeyErrors\n",
    "try:\n",
    "    for cell in padded_dict:\n",
    "        del padded_dict[cell]['capacity']\n",
    "        del padded_dict[cell]['df']\n",
    "        del padded_dict[cell]['df_subset']\n",
    "        del padded_dict[cell]['ts_cycles']\n",
    "        del padded_dict[cell]['cycle_ts_sample_count']\n",
    "        del padded_dict[cell]['ts_data']\n",
    "        del padded_dict[cell]['ts_data_thresh']\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Get rid of the string datetime data\n",
    "for cell in padded_dict:\n",
    "    for k in padded_dict[cell]['ts_padded'].keys():\n",
    "        del padded_dict[cell]['ts_padded'][k]['t']\n",
    "        \n",
    "        \n",
    "# Delete the time series arrays for the cycles with problematic time values\n",
    "try:\n",
    "    del padded_dict['035']['ts_padded'][1078]\n",
    "    del padded_dict['035']['ts_padded'][1079]\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Delete the rows of the 2D capacity array for those cycles\n",
    "cap_arr = padded_dict['035']['capacity_thresh']\n",
    "bad_cap_indices = np.array([np.where(cap_arr[:,0] == val)[0][0] for val in [1078, 1079]])\n",
    "\n",
    "if len(bad_cap_indices) != 0:\n",
    "    cap_arr = np.array([cap_arr[i] for i in range(cap_arr.shape[0]) if i not in bad_cap_indices])\n",
    "    padded_dict['035']['capacity_thresh'] = cap_arr\n",
    "\n",
    "# Delete the rows of the \"cycles_to_keep\" array for those cycles   \n",
    "cycle_arr = padded_dict['035']['ts_cycles_to_keep']\n",
    "bad_cycle_indices = np.array([np.where(cycle_arr == val)[0][0] for val in [1078, 1079]])\n",
    "    \n",
    "if len(bad_cycle_indices) != 0:\n",
    "    cycle_arr = np.array([cycle_arr[i] for i in range(cycle_arr.shape[0]) if i not in bad_cycle_indices])\n",
    "    padded_dict['035']['ts_cycles_to_keep'] = cycle_arr\n",
    "\n",
    "# Delete unnecessary variables to clean up the variable explorer\n",
    "del cap_arr, cycle_arr, bad_cap_indices, bad_cycle_indices, cell, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6fd98c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at cell 035 in padded_dict and make sure the shapes and the cycle numbers are correct \n",
    "a_cell = padded_dict['035']\n",
    "\n",
    "assert(np.all(list(a_cell['ts_padded'].keys()) == a_cell['ts_cycles_to_keep']))\n",
    "assert(np.all(a_cell['ts_cycles_to_keep'] == a_cell['capacity_thresh'][:,0].astype(int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91d6da1",
   "metadata": {},
   "source": [
    "### Min Max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8b3d6592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X and index arrays, with problematic cycle data removed\n",
    "X, index = construct_3d_x_array(padded_dict)\n",
    "\n",
    "# Get y array and squeeze to 1D\n",
    "y_arr, y_index = construct_y_soh_array(padded_dict)\n",
    "y_arr = y_arr.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767fcfe2",
   "metadata": {},
   "source": [
    "### Train, Val, Test Split\n",
    "The y arrays that are produced in this case contain the <b>capacity value</b> at a particular cycle.<br>\n",
    "This is to match what is done in the Baumhofer paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "42ef9665",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_list = list(padded_dict.keys())\n",
    "\n",
    "# Get the X and y arrays for train, validation and test\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = get_train_val_test(cell_list, X, y_arr, index, rdm_state=None)\n",
    "\n",
    "# Do per-feature min max scaling on all X arrays based on min/max values in training set.\n",
    "# Also convert np.nan values to zeros, if there are np.nan values present in the arrays.\n",
    "X_train_sc, X_val_sc, X_test_sc = min_max_scaling(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a49de",
   "metadata": {},
   "source": [
    "### These arrays are actually the wrong shape for an LSTM\n",
    "It wants [features, samples, timesteps],<br>\n",
    "but we have [samples, timesteps, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2154ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to reshape the data for LSTM\n",
    "def reshape_for_lstm(X_arr, to_plot=False):\n",
    "    # Get the shape of the data prior to reshaping for LSTM\n",
    "    features, samples, timesteps = X_arr.shape\n",
    "    \n",
    "    # Initialise an empty array to hold the reshaped X array\n",
    "    X_reshaped = np.zeros(shape=(samples, timesteps, features), dtype=float)\n",
    "    \n",
    "    for i in range(X_reshaped.shape[0]):\n",
    "        X_reshaped[i] = np.vstack([X_arr[1, i, :], X_arr[0, i, :]]).T\n",
    "        \n",
    "    if to_plot:\n",
    "        # Plot a random selection of instances to check they look OK\n",
    "        indices = np.random.randint(0, samples, size=25)\n",
    "        fig, ax = plt.subplots(5,5)\n",
    "        for subplot, sample in enumerate(indices):\n",
    "            ax.flatten()[subplot].plot(X_reshaped[sample,:,0], X_reshaped[sample,:,1], 'o')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    return X_reshaped\n",
    "\n",
    "\n",
    "X_train_lstm = reshape_for_lstm(X_train_sc)\n",
    "X_val_lstm = reshape_for_lstm(X_val_sc)\n",
    "X_test_lstm = reshape_for_lstm(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2786f039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4f3cb9e",
   "metadata": {},
   "source": [
    "### <font color='red'>These X and y arrays are now ready to be passed to a model</font>\n",
    "Probably need another function (implemented already somewhere else - dig it out) that can be used to generate train and val sets for k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9b99ff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with saving and loading these processed arrays\n",
    "parent = \"./processed_data/y_SOH/\"\n",
    "save_paths = [parent+\"train.pkl\", parent+\"validation.pkl\", parent+\"test.pkl\"]\n",
    "\n",
    "for i, tup in enumerate([(X_train_lstm, y_train), (X_val_lstm, y_val), (X_test_lstm, y_test)]):\n",
    "    with open(save_paths[i], \"wb\") as a_file:\n",
    "        pickle.dump(tup, a_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c47c0fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load back from file and see what you get\n",
    "with open(save_paths[0], \"rb\") as a_file:\n",
    "    (X_train_load, y_train_load) = pickle.load(a_file)\n",
    "    \n",
    "with open(save_paths[1], \"rb\") as a_file:\n",
    "    (X_val_load, y_val_load) = pickle.load(a_file)\n",
    "    \n",
    "with open(save_paths[2], \"rb\") as a_file:\n",
    "    (X_test_load, y_test_load) = pickle.load(a_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30dac59",
   "metadata": {},
   "source": [
    "# Summary\n",
    "The below KneeFinder stuff will ultimately be put into a new notebook, which will be more detailed.<br>\n",
    "For now, this notebook processes the data to the point where it is suitable for use with the LSTM network described in the paper (Colab link at the top of this notebook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1d64e9",
   "metadata": {},
   "source": [
    "# KneeFinder - <u>(No Longer Considering Baumhofer Paper Method)</u>\n",
    "# Put this into a new notebook\n",
    "Use KneeFinder to locate the following 5 values from each cell's capacity curve:\n",
    "- Cycle number of knee onset\n",
    "- Capacity at knee onset\n",
    "- Cycle number of knee point\n",
    "- Capacity at knee point\n",
    "- Cycle number of end of life (EOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65b13c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import medfilt\n",
    "\n",
    "# I don't like having a copy of this module in this repo. Need to find out how to have one source that is accessible everywhere.\n",
    "from knee_finder import KneeFinder\n",
    "\n",
    "# Load the params_dict from the knee_finder directory\n",
    "with open(\"./data/params_dict.pkl\", 'rb') as a_file:\n",
    "    params_dict = pickle.load(a_file)\n",
    "del a_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6bab9311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knee_and_eol_results(parent_dict, params_dict, src='baumhofer', mode='knee', filter_data=False, truncate=False, normalise=False, to_plot=False):\n",
    "    '''\n",
    "    TODO: Re-implement the truncation on a per-cell basis, maintained with a list stored as a value\n",
    "          in parent_dict[cell] under the key 'truncate_list'\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Determine data_type from \"mode\" argument\n",
    "    if mode == \"knee\":\n",
    "        data_type = \"capacity\"\n",
    "    elif mode == \"elbow\":\n",
    "        data_type = \"IR\"\n",
    "           \n",
    "\n",
    "    # Create a DataFrame whose indices are cell names\n",
    "    df = pd.DataFrame(columns=['onset', 'point', 'EOL', 'onset_y', 'point_y'], index=parent_dict.keys())\n",
    "    \n",
    "    if to_plot:\n",
    "        # Make one plot for each curve. Have a square array of subplots\n",
    "        num_plots = int(np.sqrt(len(parent_dict.keys()))) + 1\n",
    "        fig, ax = plt.subplots(num_plots, num_plots)\n",
    "\n",
    "    for i, cell in enumerate(list(parent_dict.keys())):\n",
    "        \n",
    "        # TODO - add a key \"truncate\" for a bool value that will be in each cell sub-dictionary in parent_dict\n",
    "        # this will be maintained in the same way as it was for knees_elbows_across_datasets, where we can have\n",
    "        # the sigmoid truncation logic applied on a per-cell basis, based on observing the effects on the fit and\n",
    "        # knee onset, point, EOL results.\n",
    "\n",
    "        # The code would take the following form\n",
    "#         if parent_dict[cell]['truncate'] == True:\n",
    "#             truncate = True\n",
    "#         else:\n",
    "#             truncate = False\n",
    "        \n",
    "        \n",
    "        # Get the data from the dictionary\n",
    "        arr = copy.deepcopy(parent_dict[cell]['capacity_thresh'])\n",
    "               \n",
    "        # Introduce more readable variable names\n",
    "        cycles = arr[:,0]\n",
    "        orig_values = arr[:,1]\n",
    "        \n",
    "        if normalise:\n",
    "            values = orig_values / np.max(orig_values)\n",
    "        else:\n",
    "            values = orig_values\n",
    "        \n",
    "        # Filter the data if specified\n",
    "        if filter_data:\n",
    "            values = medfilt(values, 5)\n",
    "                    \n",
    "        # Create an instance of KneeFinder\n",
    "        kf = KneeFinder(cycles, values, mode=mode, truncate=truncate)            \n",
    "        \n",
    "        # Call the KneeFinder methods to find onset, point and EOL\n",
    "        kf.set_params_using_dict(params_dict, data_type=data_type, src=src)\n",
    "        kf.find_onset_and_point()\n",
    "        kf.find_eol()\n",
    "        \n",
    "        \n",
    "        # Populate the DataFrame with the identified onset and point\n",
    "        df.loc[cell]['onset'] = kf.onset\n",
    "        df.loc[cell]['point'] = kf.point\n",
    "        df.loc[cell]['EOL'] = kf.eol_cycle\n",
    "        \n",
    "        # Get the y values on the original scale, if normalise is True\n",
    "        if normalise:\n",
    "            df.loc[cell]['onset_y'] = kf.onset_y * np.max(orig_values)\n",
    "            df.loc[cell]['point_y'] = kf.point_y * np.max(orig_values)\n",
    "            # Multiply the fit values to recover original scale\n",
    "            kf.exp_fit = kf.exp_fit * max(orig_values)\n",
    "            if truncate:\n",
    "                kf.sig_fit = kf.sig_fit * max(orig_values)\n",
    "        else:\n",
    "            df.loc[cell]['onset_y'] = kf.onset_y\n",
    "            df.loc[cell]['point_y'] = kf.point_y\n",
    "\n",
    "    \n",
    "        if to_plot:\n",
    "            ax.flatten()[i].plot(cycles, orig_values)\n",
    "\n",
    "            ax.flatten()[i].axvline(kf.onset)\n",
    "            ax.flatten()[i].axvline(kf.point)\n",
    "            ax.flatten()[i].plot(kf.x_cont[kf.indices], kf.exp_fit)\n",
    "            #if truncate:\n",
    "                #ax.flatten()[i].plot(kf.x_cont, kf.sig_fit)        \n",
    "            if kf.eol_reached:\n",
    "                ax.flatten()[i].axvline(kf.eol_cycle, color='red')\n",
    "            ax.flatten()[i].set_title(cell)\n",
    "\n",
    "    if to_plot:\n",
    "        plt.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ed506e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = get_knee_and_eol_results(padded_dict, params_dict, src='baumhofer', mode='knee', filter_data=False, truncate=True, to_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8e7eca45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x1a025d513c8>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at a particular cell in detail\n",
    "cell_ID = '010'\n",
    "cycles = np.copy(padded_dict[cell_ID]['capacity_thresh'][:,0])\n",
    "capacities = np.copy(padded_dict[cell_ID]['capacity_thresh'][:,1])\n",
    "\n",
    "kf = KneeFinder(cycles, capacities, mode='knee', truncate=True)            \n",
    "\n",
    "# Call the KneeFinder methods to find onset, point and EOL\n",
    "kf.set_params_using_dict(params_dict, data_type='capacity', src='baumhofer')\n",
    "kf.find_onset_and_point()\n",
    "kf.find_eol()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(cycles, capacities)\n",
    "ax.plot(kf.x_cont[kf.indices], kf.exp_fit)\n",
    "ax.axvline(kf.onset)\n",
    "ax.axvline(kf.point)\n",
    "ax.axvline(kf.eol_cycle, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ffd6e9",
   "metadata": {},
   "source": [
    "### Using the 5 values in the DataFrame, compute the following for every cycle:\n",
    "- Number of cycles remaining until knee onset\n",
    "- Number of cycles remaining until knee point\n",
    "- Number of cycles remaining until EOL\n",
    "- Capacity degradation (Ah) until knee onset\n",
    "- Capacity degradation (Ah) until knee point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5f6e7aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the cell IDs in the padded_dict dictionary\n",
    "cells = list(padded_dict.keys())\n",
    "\n",
    "# Create the first array as a base to be built on\n",
    "result = create_y_target_array(parent_dict=padded_dict, cell_ID=cells[0], df=df_out)\n",
    "\n",
    "# Iterate through the rest of the cells, concatenating the result to build up the array\n",
    "for cell_ID in cells[1:]:\n",
    "    temp_arr = create_y_target_array(parent_dict=padded_dict, cell_ID=cell_ID, df=df_out)\n",
    "    result = np.concatenate((result, temp_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd064971",
   "metadata": {},
   "source": [
    "## Get X and y arrays\n",
    "Apply Train, Val, Test Split and Min Max Scaling.<br>\n",
    "This will give us new sets of data, where the y arrays have 5 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "22e997de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_list = list(padded_dict.keys())\n",
    "\n",
    "# Get the X and y arrays for train, validation and test\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = get_train_val_test(cell_list, X, y=result, index=index, rdm_state=None)\n",
    "\n",
    "# Do per-feature min max scaling on all X arrays based on min/max values in training set.\n",
    "# Also convert np.nan values to zeros, if there are np.nan values present in the arrays.\n",
    "X_train_sc, X_val_sc, X_test_sc = min_max_scaling(X_train, X_val, X_test)\n",
    "\n",
    "# Because these arrays are the wrong shape for the LSTM, we need to reshape them\n",
    "X_train_lstm = reshape_for_lstm(X_train_sc)\n",
    "X_val_lstm = reshape_for_lstm(X_val_sc)\n",
    "X_test_lstm = reshape_for_lstm(X_test_sc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
