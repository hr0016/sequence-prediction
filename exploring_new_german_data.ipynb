{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring new German data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information from Philipp about the file naming:\n",
    "    \n",
    "%proj%=%spec%=%conn%=%tset%=%date%=%ptst%=%test%=%equi%=%tid%\n",
    "\n",
    "%proj%  Project Title<br>\n",
    "%spec%  Specimen Name<br>\n",
    "%conn%  Connection Name<br>\n",
    "%tset%  Testset Name<br>\n",
    "%date%  Start Date<br>\n",
    "%ptst%  Parent Test Name (empty if not existing)<br>\n",
    "%test%  Test Name<br>\n",
    "%equi%  Equipment Name<br>\n",
    "%tid%   Unique Id for Test based on data location\n",
    "        (available if test was imported to ahjo)<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time\n"
     ]
    }
   ],
   "source": [
    "# Set up the translation into English for the column headers\n",
    "# German column headers\n",
    "german = [\"Schritt\",\"Zustand\",\"Zeit\",\"Programmdauer\",\"Schrittdauer\",\"Zyklus\",\n",
    "          \"Zyklusebene\",\"Prozedur\",\"Prozedurebene\",\"AhAkku\",\"AhLad\",\"AhEla\",\n",
    "          \"AhStep\",\"Energie\",\"WhStep\",\"Spannung\",\"Strom\",\"Temp13\"]\n",
    "\n",
    "# English translations\n",
    "english = [\"step\", \"state\", \"time\", \"programme duration\", \"step duration\",\n",
    "           \"cycle\", \"cycle level\", \"procedure\", \"procedure level\", \"Qacc\",\n",
    "           \"Qcha\", \"Qdch\", \"AhStep\", \"energy\", \"WhStep\", \"voltage\",\n",
    "           \"current\", \"temp13\"]\n",
    "\n",
    "# Check list lengths match\n",
    "assert(len(german) == len(english))\n",
    "\n",
    "# Create a dictionary and view a test entry\n",
    "translate = dict(zip(german, english))\n",
    "print(translate['Zeit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where the CSV files are stored and get a list of their paths\n",
    "file_dir = \"D:/Dropbox/UoE_Batteries/new_german_data/\"\n",
    "files = glob.glob(file_dir + \"**/*.csv\", recursive=True)\n",
    "# Make a list of just the CSV file names (not paths) for easier file locating\n",
    "csv_names = [file.split(\"\\\\\")[1] for file in files]\n",
    "\n",
    "\n",
    "# Specify a converter dictionary for use with pd.read_csv, to specify data types\n",
    "# of columns contained within the CSV. Use the original German column names.\n",
    "# Need to find out what to do with the time column. Leave as \"object\" for now.\n",
    "dtypes = [int, str, object, float, float, int, int, str, int,\n",
    "          float, float, float, float, float, float, float, float, float]\n",
    "\n",
    "converter = dict(zip(german, dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a function to read the data from a file and handle the translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_csv(fpath, converter, fields=None, translation=True): \n",
    "    '''\n",
    "    Load data from CSV files provided by Philipp at Aachen.\n",
    "    Handle data type conversion, loading a specific set of columns and translating column headers.\n",
    "    \n",
    "    Inputs:\n",
    "        fpath (type: str)\n",
    "            Path to the CSV file you want to load\n",
    "        \n",
    "        converter (type: dict)\n",
    "            Dict to map data in columns to desired data types.\n",
    "            Keys: German column names.\n",
    "            Values: Data type of column\n",
    "        \n",
    "        fields (type: list)\n",
    "            A list of German column names that should be loaded from the CSV file\n",
    "            \n",
    "        translation (type: bool, default: True)\n",
    "            Whether or not to translate the German column headers into English\n",
    "            (requires a dict called \"translate\", defined outside the scope of this function)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    df = pd.read_csv(fpath, skiprows=[1], header=0, dtype=converter)\n",
    "    \n",
    "    # Initialise variable name so there's something to return irrespective of translation bool state\n",
    "    translation_error_log = None\n",
    "    \n",
    "    # Go through translation routine if required. Check for failed translations.\n",
    "    if translation:\n",
    "        # Translate German columns where a translation exists in the dictionary, else leave the German.\n",
    "        translated_cols = [translate[ger_col] if ger_col in translate.keys() else ger_col for ger_col in df.columns]\n",
    "        \n",
    "        # Store any column names that haven't been translated\n",
    "        failed_translations = np.where([col not in translate.values() for col in translated_cols])[0]\n",
    "        \n",
    "        # This condition fails if len(failed_translations)==0\n",
    "        if np.any(failed_translations):\n",
    "            # Add the file path, as well as all failed translations and their column index\n",
    "            translation_error_log = [fpath, [(idx, df.columns[idx]) for idx in failed_translations]]\n",
    "        \n",
    "        # Replace the column names with the translated names\n",
    "        df.columns = translated_cols\n",
    "        \n",
    "    \n",
    "    # Get rid of null rows, if present\n",
    "    df.dropna(inplace=True)\n",
    "    # Reset the indices in case of null row deletion\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return df, translation_error_log\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an example file using the function\n",
    "data_from_fn, _ = load_from_csv(files[2], converter, translation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the load_from_csv function and examine translation results\n",
    "Some files have additional columns. Find all unique column names and see if we need these extra ones. If so, add the German and English to the translate dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [00:08,  7.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialise a list to store filepaths for files that raise an exception inside the function\n",
    "failed_files = []\n",
    "# Initialise a set to store unique German column names\n",
    "unique_columns = set()\n",
    "\n",
    "for i, f in tqdm(enumerate(files)):\n",
    "    try:\n",
    "        # Set translation to False so we get the German column names\n",
    "        data, trans_error_log = load_from_csv(f, converter, translation=False)\n",
    "        # Get the German column names and add them to the set\n",
    "        for col in data.columns:\n",
    "            # Add every column name from every file to the set\n",
    "            unique_columns.add(col)\n",
    "            \n",
    "    except:\n",
    "        e = sys.exc_info()[1]\n",
    "        failed_files.append([f, e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Temp23', 'Temp0029', 'ActTemp', 'ClimaEN', 'Temp0028', 'Temp', 'SetTemp', 'Temp0030', 'Agilent', 'ClimaOn', 'Temp0027']\n"
     ]
    }
   ],
   "source": [
    "# Now let's look at the column names that are not already contained in our \"german\" list\n",
    "# Get the column names that are present in both \"unique_columns\" and \"german\" variables\n",
    "intersection = np.intersect1d(list(unique_columns), german)\n",
    "\n",
    "# Find the column names in \"unique_columns\" but NOT in \"german\".\n",
    "# symmetric_difference is a method of the set class. It returns a set\n",
    "new_cols = list(unique_columns.symmetric_difference(german))\n",
    "\n",
    "# We can see that for this first batch of files, at least, these additional columns\n",
    "# don't seem to be important for us. They are mostly related to temperatures.\n",
    "# TODO - find out what Agilent is. Translator doesn't work.\n",
    "print(new_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the data from an example file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88320 rows\n",
      "   step state                    time  ...   voltage   current   temp13\n",
      "0     4   DCH  2013-01-24 16:50:17.26  ...  3.482481 -3.359215  27.4375\n",
      "1     4   DCH  2013-01-24 16:50:17.34  ...  3.504115 -2.740188  27.4375\n",
      "2     4   DCH  2013-01-24 16:50:17.34  ...  3.504115 -2.740188  27.4375\n",
      "3     4   DCH  2013-01-24 16:50:17.39  ...  3.500815 -2.811177  27.4375\n",
      "4     4   DCH  2013-01-24 16:50:17.66  ...  3.500815 -2.760065  27.4375\n",
      "\n",
      "[5 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "data, _ = load_from_csv(files[4], converter, translation=True)\n",
    "print(f\"{len(data)} rows\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   4    5    7 9999]\n",
      "['DCH' 'CHA' 'STO']\n",
      "4\n",
      "   step state                   time  ...  voltage   current   temp13\n",
      "0     4   DCH  2013-01-16 09:08:22.5  ...  3.52135 -2.433514  24.3125\n",
      "\n",
      "[1 rows x 18 columns]\n",
      "\n",
      "5\n",
      "     step state                    time  ...   voltage   current   temp13\n",
      "238     5   CHA  2013-01-16 09:38:22.98  ...  3.504076  0.004257  27.0625\n",
      "\n",
      "[1 rows x 18 columns]\n",
      "\n",
      "7\n",
      "       step state                    time  ...   voltage  current   temp13\n",
      "88787     7   STO  2013-01-23 01:11:23.88  ...  3.898142      0.0  27.6875\n",
      "\n",
      "[1 rows x 18 columns]\n",
      "\n",
      "9999\n",
      "       step state                    time  ...   voltage  current   temp13\n",
      "88788  9999   STO  2013-01-23 01:11:23.88  ...  3.898142      0.0  27.6875\n",
      "\n",
      "[1 rows x 18 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the unique step values\n",
    "print(data['step'].unique())\n",
    "\n",
    "# Find the unique state values\n",
    "print(data['state'].unique())\n",
    "\n",
    "# Look at the state value for each of these steps\n",
    "for step_num in data['step'].unique():\n",
    "    temp_df = data[data['step'] == step_num]\n",
    "    print(step_num)\n",
    "    print(temp_df.head(1))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some data\n",
    "# Get a DataFrame for a CHA step\n",
    "# cha_df = data[data['state']=='CHA']\n",
    "# V = cha_df['voltage'].to_numpy()\n",
    "# I = cha_df['current'].to_numpy()\n",
    "\n",
    "V = data['voltage'].to_numpy()\n",
    "I = data['current'].to_numpy()\n",
    "temp_df = data[data['step'] != 9999]\n",
    "steps = temp_df['step']\n",
    "\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "#ax1.set_xlabel('time (s)')\n",
    "#ax1.set_ylabel('Current and Voltage', color=color)\n",
    "ax1.plot(V, label='Voltage')\n",
    "ax1.plot(I, label='Current')\n",
    "ax1.set_ylim([-10, 10])\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.4)\n",
    "#ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Step')  # we already handled the x-label with ax1\n",
    "ax2.set_ylim([0, 200])\n",
    "ax2.plot(steps, color='black', label='Step', alpha=0.7)\n",
    "#ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(V, label='Voltage')\n",
    "# ax.plot(I, label='Current')\n",
    "# ax.plot(steps, label='Step')\n",
    "# ax.legend()\n",
    "# ax.grid(alpha=0.5)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the number of cycles present in the files that contain repeated discharge-charge processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the saved plots, we see that the following files are dch-cha repeated\n",
    "cycle_file_indices = [2, 4, 6, 8, 10, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31]\n",
    "num_cycles = [None for i in range(len(cycle_file_indices))]\n",
    "cycles_per_file = dict(zip(cycle_file_indices, num_cycles))\n",
    "\n",
    "# Load each of the files\n",
    "for idx in cycle_file_indices:\n",
    "    data, _ = load_from_csv(files[idx], converter, translation=True)\n",
    "    cycles_per_file[idx] = np.max(data['cycle'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some voltage and current data from one of the files that has repeated cycles\n",
    "data, _ = load_from_csv(files[2], converter, translation=True)\n",
    "#subset = data[(data.cycle >= 1) & (data.cycle <= 6)]\n",
    "subset = data[data['cycle'] == 1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(subset['voltage'], color='blue', label='Voltage')\n",
    "ax.set_ylim([3, 4])\n",
    "ax.legend(loc=3)\n",
    "\n",
    "ax2 = ax.twinx() \n",
    "ax2.plot(subset['current'], color='orange', label='Current')\n",
    "ax2.set_ylim([-5, 5])\n",
    "ax2.legend(loc=4)\n",
    "ax2.grid(alpha=0.4)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at how the current and voltage profiles change over the course of N cycles.\n",
    "### Each file contains 160 cycles, so we could look at the first cycle from each file and plot them to see if there's a shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the files for cell 18\n",
    "cell_18_files = [f for f in files if \"018=\" in f]\n",
    "# Make a dictionary to store the current and voltage data\n",
    "cycle_data = {k: {'V': None, 'I': None} for k in cycle_file_indices}\n",
    "\n",
    "for idx in list(cycle_data.keys()):\n",
    "    data, _ = load_from_csv(cell_18_files[idx], converter, translation=True)\n",
    "    # Take a subset of the DataFrame\n",
    "    data = data[data['cycle'] == 1]\n",
    "    cycle_data[idx]['V'] = data['voltage'].to_numpy().astype(float)\n",
    "    cycle_data[idx]['I'] = data['current'].to_numpy().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the currents\n",
    "fig, ax = plt.subplots()\n",
    "for k in list(cycle_data.keys()):\n",
    "    ax.plot(cycle_data[k]['I'], label=k)\n",
    "\n",
    "ax.grid(alpha=0.4)    \n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the voltages\n",
    "fig, ax = plt.subplots()\n",
    "for k in list(cycle_data.keys()):\n",
    "    ax.plot(cycle_data[k]['V'], label=k)\n",
    "\n",
    "ax.grid(alpha=0.4)    \n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at deaing with the time information. Interpolate so it's on a regularly spaced time reference\n",
    "import datetime\n",
    "\n",
    "data, _ = load_from_csv(cell_18_files[2], converter, translation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06\n"
     ]
    }
   ],
   "source": [
    "# dt1 = datetime.datetime.strptime(data.loc[0]['time'], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "# dt2 = datetime.datetime.strptime(data.loc[1]['time'], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "# elapsed_time = dt2 - dt1\n",
    "# print(elapsed_time.total_seconds())\n",
    "\n",
    "# Make a new DataFrame that just contains the time, for experimentation and investigate vectorising the\n",
    "# function to compute the number of seconds at each measurement\n",
    "t_data = data['time']\n",
    "time_df = pd.DataFrame(t_data)\n",
    "# Create an empty column for the time in seconds, starting at zero\n",
    "#time_df['t'] = np.nan\n",
    "\n",
    "# Note - avoid chained indexing e.g. time.loc[0]['t']\n",
    "#time_df.at[0, 't'] = 0.0\n",
    "\n",
    "# Get pandas datetime objects\n",
    "datetimes = pd.to_datetime(time_df['time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-01-16 09:08:22.500000\n",
      "576181.38\n"
     ]
    }
   ],
   "source": [
    "def grimace_function(df):\n",
    "    \n",
    "    # Get a datetime object for the first entry in the time column\n",
    "    row1 = df.iloc[0]\n",
    "    dt1 = datetime.datetime.strptime(row1['time'], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    \n",
    "    # Shouldn't use a loop, but do it for experimentation\n",
    "    for i in df.index:\n",
    "        if i == 0:\n",
    "            continue\n",
    "        dt_val = datetime.datetime.strptime(df.at[i, 'time'], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        elapsed_time = dt_val - dt1\n",
    "        elapsed_time = elapsed_time.total_seconds()\n",
    "        df.at[i, 't'] = elapsed_time\n",
    "        \n",
    "    return df\n",
    "    \n",
    "    \n",
    "grimace_function(time_df)\n",
    "\n",
    "dt1 = datetime.datetime.strptime(time_df['time'].iloc[0], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "dt2 = datetime.datetime.strptime(time_df['time'].iloc[-1], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "elapsed_time = dt2 - dt1\n",
    "print(elapsed_time.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datestr_to_datetime(x):\n",
    "    return datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "\n",
    "def timedelta_to_float(x):\n",
    "    return x.total_seconds()\n",
    "\n",
    "\n",
    "# Vectorise the functions\n",
    "convert_time = np.vectorize(datestr_to_datetime)\n",
    "get_seconds = np.vectorize(timedelta_to_float)\n",
    "\n",
    "\n",
    "time_df = pd.DataFrame(data['time'])\n",
    "datetimes = pd.to_datetime(time_df['time'])\n",
    "\n",
    "result = convert_time(datetimes.astype(str))\n",
    "\n",
    "# Get the array of times elapsed since the first row's time stamp\n",
    "elapsed = result - result[0]   # Type is timedelta\n",
    "\n",
    "time_df['elapsed'] = get_seconds(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "13\n",
      "15\n",
      "17\n",
      "19\n",
      "21\n",
      "23\n",
      "25\n",
      "27\n",
      "29\n",
      "31\n",
      "27.159066200256348\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def datestr_to_datetime(x):\n",
    "    return datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "\n",
    "def timedelta_to_float(x):\n",
    "    return x.total_seconds()\n",
    "\n",
    "\n",
    "# Vectorise the functions\n",
    "convert_time = np.vectorize(datestr_to_datetime)\n",
    "get_seconds = np.vectorize(timedelta_to_float)\n",
    "\n",
    "\n",
    "def get_elapsed_time_seconds(df):\n",
    "    #print(\"in get_elapsed_time_seconds\")\n",
    "    # Extract the time column and store as a DataFrame\n",
    "    time_df = pd.DataFrame(df['time'])\n",
    "    # Convert the entries to pandas Timestamp object\n",
    "    #print(\"Converting to pandas Timestamp objects\")\n",
    "    datetimes = pd.to_datetime(time_df['time'])\n",
    "    # Convert to Python datetime object to use the total_seconds() method\n",
    "    #print(\"Converting to python datetime objects\")\n",
    "    py_datetimes = convert_time(datetimes.astype(str))\n",
    "    # Get the array of times elapsed since the first row's time stamp\n",
    "    #print(\"Getting elapsed time\")\n",
    "    elapsed = py_datetimes - py_datetimes[0]   # Type is timedelta\n",
    "    # Use the vectorised function to convert all the timedelta objects to floats\n",
    "    seconds_array = get_seconds(elapsed)\n",
    "    \n",
    "    return seconds_array\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for idx in cycle_file_indices:\n",
    "    print(idx)\n",
    "    # Load the data\n",
    "    data, _ = load_from_csv(cell_18_files[idx], converter, translation=True)\n",
    "    # Add a column for elapsed time in seconds\n",
    "    data['elapsed'] = None\n",
    "\n",
    "    # Remove instances of cycle number zero - these occur at the end of each file\n",
    "    data = data[data['cycle'] != 0]\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Get the cycle numbers present\n",
    "    cycles = data['cycle'].unique()\n",
    "\n",
    "    for i, cycle in enumerate(cycles):\n",
    "\n",
    "        select_indices = list(np.where(data[\"cycle\"] == cycle)[0])\n",
    "        elapsed_array = get_elapsed_time_seconds(data.iloc[select_indices])\n",
    "        #elapsed_array = get_elapsed_time_seconds(subset)\n",
    "        data.loc[data.cycle==cycle, 'elapsed'] = elapsed_array\n",
    "        #data.iloc[select_indices, 'elapsed'] = elapsed_array\n",
    "    \n",
    "    \n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the DataFrame for just one cycle and experiment with the interpolation\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "cycle_one = pd.DataFrame(df[df.cycle == 1])\n",
    "\n",
    "# Get numpy arrays for elapsed time, voltage and current\n",
    "elapsed_arr = cycle_one['elapsed'].to_numpy()\n",
    "voltage_arr = cycle_one['voltage'].to_numpy()\n",
    "current_arr = cycle_one['current'].to_numpy()\n",
    "\n",
    "# Instantiate a regularly spaced time array\n",
    "regular_time = np.arange(0, np.max(elapsed_arr), 1)\n",
    "f_i = interp1d(elapsed_arr, current_arr)\n",
    "f_v = interp1d(elapsed_arr, voltage_arr)\n",
    "\n",
    "interp_current = f_i(regular_time)\n",
    "interp_voltage = f_v(regular_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(regular_time, interp_current, color='blue', label='Interp. current')\n",
    "ax.plot(elapsed_arr, current_arr, 'o', color='purple', label='Experimental current')\n",
    "ax.set_ylim([-5, 5])\n",
    "ax.legend(loc=3)\n",
    "\n",
    "ax2 = ax.twinx() \n",
    "ax2.plot(regular_time, interp_voltage, color='orange', label='Interp. voltage')\n",
    "ax2.plot(elapsed_arr, voltage_arr, 'o', color='red', label='Experimental voltage')\n",
    "ax2.set_ylim([3, 4])\n",
    "ax2.legend(loc=4)\n",
    "ax2.grid(alpha=0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datetime.datetime'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def grimace_vectorised(df):\n",
    "    '''\n",
    "    Write the steps:\n",
    "    * Create a new column containing datetime objects from the 'time' column\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get a datetime object for the first entry in the time column\n",
    "    row1 = df.iloc[0]\n",
    "    dt1 = datetime.datetime.strptime(row1['time'], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    \n",
    "    # Get an array of strings\n",
    "    string_array = df['time'].to_numpy()\n",
    "    \n",
    "    # Try applying datetime to it\n",
    "    \n",
    "    print(type(dt1))\n",
    "    print(type(row1['time']))\n",
    "    #df = df.assign(date_times = lambda x: )\n",
    "    #df = df.assign(t_vectorised = lambda x: (datetime.datetime.strptime(x['time'], \"%Y-%m-%d %H:%M:%S.%f\") - dt1))\n",
    "    \n",
    "bilge_df = grimace_vectorised(time_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20e695eaec8>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = data[data['cycle'] == 1]\n",
    "plt.plot(subset['step duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cycles:  1\n",
      "Min. cycle number:  0\n",
      "Max cycle number:  0\n"
     ]
    }
   ],
   "source": [
    "# Find out how many cycles there are\n",
    "print(\"Number of cycles: \", len(cha_df['cycle'].unique()))\n",
    "print(\"Min. cycle number: \", np.min(cha_df['cycle']))\n",
    "print(\"Max cycle number: \", np.max(cha_df['cycle']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from a particular step\n",
    "step_44_data = data[data['step']==44]\n",
    "V = step_44_data['voltage'].to_numpy()\n",
    "I = step_44_data['current'].to_numpy()\n",
    "Q = step_44_data['Qcha'].to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot(V)\n",
    "ax[0].plot(I)\n",
    "ax[1].plot(Q)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
