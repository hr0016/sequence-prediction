{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f7a111",
   "metadata": {},
   "source": [
    "# Baumhofer Data - First 100 Cycles Only, With Time Series Interpolation\n",
    "For the Baumhofer data, we consider only the first 100 cycles for each cell. The objective is to use as input <b>one cycle of data</b> and predict the following targets:\n",
    "\n",
    "- number of cycles until knee onset\n",
    "- number of cycles until knee point\n",
    "- number of cycles until end of life (EOL)\n",
    "- the capacity degradation that will occur between the capacity at input cycle and the capacity at knee onset\n",
    "- the capacity degradation that will occur between the capacity at input cycle and the capacity at knee point\n",
    "\n",
    "The focus is on <u>early prediction</u>, meaning we train using data taken from the first 100 cycles, and we predict within the first 100 cycles also.<br>\n",
    "\n",
    "An additional time series array (Q) is constructed by integrating the current over time. Rather than using the arrays of voltage and elapsed time as inputs (as was done with Weihan Li's LSTM paper), the time series arrays are resampled to be regularly spaced in time.<br>\n",
    "\n",
    "This notebook is kept separate to the notebook that considers the \"full\" data (all cycles per cell), because there are a lot of cleaning steps that are necessary for the full data, but not for the first 100 cycles. This prevents wasted processing time. As such, some of the functions defined in \"baumhofer_utils.py\" are not utilised in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99acf51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import cumtrapz\n",
    "from scipy.signal import medfilt\n",
    "import tensorflow as tf\n",
    "\n",
    "from baumhofer_utils import *\n",
    "from knee_finder import KneeFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb61dff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some constants\n",
    "# The time interval, in seconds, between the interpolated measurements\n",
    "TIME_FREQ = 5\n",
    "# Keep only the first N cycles for every cell. Do it here to avoid wasted processing.\n",
    "first_n_cycles = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d287854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the params_dict from the knee_finder directory\n",
    "with open(\"./data/params_dict.pkl\", 'rb') as a_file:\n",
    "    params_dict = pickle.load(a_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e521fd",
   "metadata": {},
   "source": [
    "### Load the saved dictionary containing the first 100 cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9cb8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/baumhofer_first_100_cycles.pkl\", \"rb\") as a_file:\n",
    "    data = pickle.load(a_file)\n",
    "    \n",
    "del a_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac90d7d",
   "metadata": {},
   "source": [
    "### Interpolate the time series data so the samples are regularly spaced in time\n",
    "This is basically the code from Paula's \"utils.py\" module, used for Severson pre-processing.<br>\n",
    "However, it operates on a single array, rather than a full dictionary.<br>\n",
    "\n",
    "This is done intentionally, to decouple the function from the dictionary's structure and/or nomenclature.<br>\n",
    "A dictionary containing interpolated arrays can be built using repeated calls to this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab15733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_array(arr, time_arr, integrate=False, time_freq=1):\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arr (type: numpy.ndarray)\n",
    "        An array of measured values corresponding to the measurement times in time_arr.\n",
    "        \n",
    "    time_arr (type: numpy.ndarray)\n",
    "        An array of time values at which the measurements in arr were recorded.\n",
    "        \n",
    "    integrate (type: bool)\n",
    "        If True, compute the integral of arr over time_arr using scipy.integrate.cumtrapz().\n",
    "        This should be used if you want to obtain charge values via Coulomb counting, with\n",
    "        the input arr being an array of current values.\n",
    "        \n",
    "    time_freq (type: int or float)\n",
    "        The time interval to use between samples in the interpolated array.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    regular_time (type: numpy.ndarray)\n",
    "        Array of regularly spaced time values\n",
    "    \n",
    "    interp_arr (type: numpy.ndarray)\n",
    "        Array of interpolated values\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if integrate:\n",
    "        # Create an array of values integrated over time values in time_arr.\n",
    "        # Insert a zero value at the first index\n",
    "        arr = np.insert(cumtrapz(arr, time_arr), 0, 0)\n",
    "        \n",
    "    # Create an array of regularly spaced times\n",
    "    regular_time = np.arange(0, np.max(time_arr), time_freq)\n",
    "    \n",
    "    # Use interp1d to return a function that approximates y = f(x)\n",
    "    interp_func = interp1d(time_arr, arr)\n",
    "    # Use the function to compute array values at the points in regular_time array\n",
    "    interp_arr = interp_func(regular_time)\n",
    "    \n",
    "    return regular_time, interp_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf34ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:02<00:00, 23.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a new sub-dictionary inside the data dictionary, to store interpolated data\n",
    "\n",
    "for cell in data:\n",
    "    data[cell]['interp_data'] = {cycle: {'V': None, 'I': None, 'T': None, 'Q': None, 't_regular': None} for cycle in data[cell]['ts_data']}\n",
    "    \n",
    "# Initialise a variable to find out the length of the longest interpolated array\n",
    "max_len = 0\n",
    "\n",
    "# Call the interpolation function for every cycle\n",
    "for cell in tqdm(data):\n",
    "    for cycle in data[cell]['interp_data']:\n",
    "        # Get the elapsed time array\n",
    "        time_arr = data[cell]['ts_data'][cycle]['t_elapsed']\n",
    "\n",
    "        # Get the measurement arrays\n",
    "        V_arr = data[cell]['ts_data'][cycle]['V']\n",
    "        I_arr = data[cell]['ts_data'][cycle]['I']\n",
    "        T_arr = data[cell]['ts_data'][cycle]['T']\n",
    "        \n",
    "        # Compute interpolated arrays and assign to dictionary\n",
    "        data[cell]['interp_data'][cycle]['t_regular'], data[cell]['interp_data'][cycle]['V'] = interpolate_array(arr=V_arr, time_arr=time_arr, integrate=False, time_freq=TIME_FREQ)\n",
    "        _, data[cell]['interp_data'][cycle]['I'] = interpolate_array(arr=I_arr, time_arr=time_arr, integrate=False, time_freq=TIME_FREQ)\n",
    "        _, data[cell]['interp_data'][cycle]['T'] = interpolate_array(arr=T_arr, time_arr=time_arr, integrate=False, time_freq=TIME_FREQ)\n",
    "        _, data[cell]['interp_data'][cycle]['Q'] = interpolate_array(arr=I_arr, time_arr=time_arr, integrate=True, time_freq=TIME_FREQ)\n",
    "        \n",
    "        if len(data[cell]['interp_data'][cycle]['I']) > max_len:\n",
    "            max_len = len(data[cell]['interp_data'][cycle]['I'])\n",
    "            \n",
    "            \n",
    "# Delete the unnecessary variables to keep the explorer clean\n",
    "del cell, cycle, time_arr, V_arr, I_arr, T_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f3132",
   "metadata": {},
   "source": [
    "### Do we need to pad the data?\n",
    "Find the number of unique array lengths that occur in the dataset of interpolated time series arrays.<br>\n",
    "If there is only one unique array length, then we have no requirement to pad the data to max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28d9629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out how many unique array lengths there are in the interpolated data\n",
    "unique_arr_lengths = set()\n",
    "\n",
    "for cell in data:\n",
    "    for cycle in data[cell]['interp_data']:\n",
    "        arr_len = len(data[cell]['interp_data'][cycle]['V'])\n",
    "        unique_arr_lengths.add(arr_len)\n",
    "        \n",
    "del cell, cycle, arr_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b8920",
   "metadata": {},
   "source": [
    "### Pad the time series arrays to the max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8f0f47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No need to pad interpolated data, since all arrays are already the same length: {721}\n"
     ]
    }
   ],
   "source": [
    "if len(unique_arr_lengths) > 1:\n",
    "    # Specify a mask value to be used for padding.\n",
    "    # Use NAN so normalisation is not affected.\n",
    "    mask_val = np.nan\n",
    "\n",
    "    # Create a sub-dictionary in data dictionary to store the interpolated arrays that have been nan-padded\n",
    "    for cell in data:\n",
    "        data[cell]['padded_data'] = {cycle: {'V': None, 'I': None, 'T': None, 'Q': None} for cycle in data[cell]['interp_data']}\n",
    "\n",
    "    # Loop through the time series arrays and extend them with np.nan, to have length of max_len\n",
    "    for cell in tqdm(data):\n",
    "        for cycle in data[cell]['interp_data']:\n",
    "            # For each variable e.g. voltage, time\n",
    "            for var in data[cell]['interp_data'][cycle]:\n",
    "                # Store the data in a variable for readability\n",
    "                arr = data[cell]['interp_data'][cycle][var]\n",
    "                # Find the current length of the array\n",
    "                current_len = arr.shape[0]\n",
    "                # Specify how many padded values to add\n",
    "                num_pad_vals_to_add = max_len - current_len\n",
    "\n",
    "                # Generate the padded array\n",
    "                padded_arr = np.append(arr, np.repeat(mask_val, num_pad_vals_to_add))\n",
    "\n",
    "                # Assign it to the relevant location in padded_cell_dict\n",
    "                data[cell]['padded_data'][cycle][var] = padded_arr\n",
    "\n",
    "\n",
    "    # Delete the unnecessary variables to keep the explorer clean\n",
    "    del cell, cycle, arr, padded_arr, current_len, num_pad_vals_to_add\n",
    "    \n",
    "else:\n",
    "    print(f\"No need to pad interpolated data, since all arrays are already the same length: {unique_arr_lengths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7760f761",
   "metadata": {},
   "source": [
    "### Create an X array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aece6f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_3d_x_array(input_dict, variables=['V', 't_elapsed'], key='ts_padded'):\n",
    "    '''\n",
    "    \n",
    "    The resulting array has the shape [num_features, num_cycles, num_samples],\n",
    "    where:\n",
    "        num_features refers to voltage and elapsed cycle time (seconds),\n",
    "        num_cycles refers to the number of cycles over all cells,\n",
    "        num_samples refers to the number of measurements taken within a cycle\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dict (type: dict)\n",
    "        Description\n",
    "        \n",
    "    \n",
    "    variables (type: list)\n",
    "        Description    \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_arr (type: numpy array)\n",
    "        Description\n",
    "        \n",
    "    \n",
    "    cell_cycle_indices (type: numpy array)\n",
    "        Description\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get a list of cells from the input dict keys\n",
    "    cells = list(input_dict.keys())\n",
    "    \n",
    "    # Initialise the 3D array using the padded time series data from the first cell\n",
    "    X_arr = np.array([[input_dict[cells[0]][key][cycle][var] for cycle in input_dict[cells[0]][key].keys()] for var in variables])\n",
    "    \n",
    "    # Initialise a 1D array that will tell us which cell the cycle came from\n",
    "    #cell_cycle_indices = np.array([str(cells[0]) for i in range(X_arr.shape[1])])\n",
    "    cell_cycle_indices = np.array([str(cells[0])+\"_\"+str(k) for k in input_dict[cells[0]][key].keys()])\n",
    "    \n",
    "    # Build up the complete arrays by creating the arrays per cell, then appending them to the master arrays\n",
    "    for cell in cells[1:]:\n",
    "        temp_arr = np.array([[input_dict[cell][key][cycle][var] for cycle in input_dict[cell][key].keys()] for var in variables])\n",
    "        X_arr = np.append(X_arr, temp_arr, axis=1)\n",
    "\n",
    "        #temp_indices_arr = np.array([str(cell) for i in range(temp_arr.shape[1])])\n",
    "        temp_indices_arr = np.array([str(cell)+\"_\"+str(k)  for k in input_dict[cell][key].keys()])\n",
    "        cell_cycle_indices = np.append(cell_cycle_indices, temp_indices_arr)\n",
    "        \n",
    "        \n",
    "    return X_arr, cell_cycle_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fca852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the data needed to be padded, change the \"key\" argument to \"padded_data\"\n",
    "X, index = construct_3d_x_array(input_dict=data, variables=['V', 'I', 'Q'], key='interp_data')\n",
    "\n",
    "# We see that the first values for current and voltage in the time series array for a given cycle\n",
    "# are strange. Let's delete them and keep the rest.\n",
    "X = np.delete(X, 0, axis=2)\n",
    "\n",
    "# Also, the first cycle for each cell seems to have dodgy values. Get rid of these too.\n",
    "bad_indices = [i for i in range(X.shape[1]) if i%first_n_cycles == 0]\n",
    "X = np.delete(X, bad_indices, axis=1)\n",
    "\n",
    "# Remove the same cycles from the index_orig array\n",
    "index = np.delete(index, bad_indices)\n",
    "\n",
    "del bad_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73061094",
   "metadata": {},
   "source": [
    "### Generate Targets - Use KneeFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65b1bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knee_and_eol_results(parent_dict, params_dict, src='baumhofer', mode='knee', filter_data=False, truncate=False, normalise=False, to_plot=False):\n",
    "    '''\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Determine data_type from \"mode\" argument\n",
    "    if mode == \"knee\":\n",
    "        data_type = \"capacity\"\n",
    "    elif mode == \"elbow\":\n",
    "        data_type = \"IR\"\n",
    "           \n",
    "\n",
    "    # Create a DataFrame whose indices are cell names\n",
    "    df = pd.DataFrame(columns=['onset', 'point', 'EOL', 'onset_y', 'point_y'], index=parent_dict.keys())\n",
    "    \n",
    "    if to_plot:\n",
    "        # Make one plot for each curve. Have a square array of subplots\n",
    "        num_plots = int(np.sqrt(len(parent_dict.keys()))) + 1\n",
    "        fig, ax = plt.subplots(num_plots, num_plots)\n",
    "\n",
    "    for i, cell in enumerate(list(parent_dict.keys())):\n",
    "                \n",
    "        # Get the capacity data from the dictionary\n",
    "        arr = copy.deepcopy(parent_dict[cell]['capacity'])\n",
    "               \n",
    "        # Introduce more readable variable names\n",
    "        cycles = arr[:,0]\n",
    "        orig_values = arr[:,1]\n",
    "        \n",
    "        if normalise:\n",
    "            values = orig_values / np.max(orig_values)\n",
    "        else:\n",
    "            values = orig_values\n",
    "        \n",
    "        # Filter the data if specified\n",
    "        if filter_data:\n",
    "            values = medfilt(values, 5)\n",
    "                    \n",
    "        # Create an instance of KneeFinder\n",
    "        kf = KneeFinder(cycles, values, mode=mode, truncate=truncate)            \n",
    "        \n",
    "        # Call the KneeFinder methods to find onset, point and EOL\n",
    "        kf.set_params_using_dict(params_dict, data_type=data_type, src=src)\n",
    "        kf.find_onset_and_point()\n",
    "        kf.find_eol()\n",
    "        \n",
    "        # Populate the DataFrame with the identified onset and point\n",
    "        df.loc[cell]['onset'] = kf.onset\n",
    "        df.loc[cell]['point'] = kf.point\n",
    "        df.loc[cell]['EOL'] = kf.eol_cycle\n",
    "        \n",
    "        # Get the y values on the original scale, if normalise is True\n",
    "        if normalise:\n",
    "            df.loc[cell]['onset_y'] = kf.onset_y * np.max(orig_values)\n",
    "            df.loc[cell]['point_y'] = kf.point_y * np.max(orig_values)\n",
    "            # Multiply the fit values to recover original scale\n",
    "            kf.exp_fit = kf.exp_fit * max(orig_values)\n",
    "            if truncate:\n",
    "                kf.sig_fit = kf.sig_fit * max(orig_values)\n",
    "        else:\n",
    "            df.loc[cell]['onset_y'] = kf.onset_y\n",
    "            df.loc[cell]['point_y'] = kf.point_y\n",
    "\n",
    "    \n",
    "        if to_plot:\n",
    "            ax.flatten()[i].plot(cycles, orig_values)\n",
    "\n",
    "            ax.flatten()[i].axvline(kf.onset)\n",
    "            ax.flatten()[i].axvline(kf.point)\n",
    "            ax.flatten()[i].plot(kf.x_cont[kf.indices], kf.exp_fit)\n",
    "            #if truncate:\n",
    "                #ax.flatten()[i].plot(kf.x_cont, kf.sig_fit)        \n",
    "            if kf.eol_reached:\n",
    "                ax.flatten()[i].axvline(kf.eol_cycle, color='red')\n",
    "            ax.flatten()[i].set_title(cell)\n",
    "\n",
    "    if to_plot:\n",
    "        plt.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4a037fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = get_knee_and_eol_results(data, params_dict, src='baumhofer', mode='knee', filter_data=False, truncate=True, to_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6a4ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there are no nan values in df_out\n",
    "assert(df_out.isnull().values.any() == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e48774",
   "metadata": {},
   "source": [
    "### Create the target (y) array of 5 values per cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "157c72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_y_target_array(parent_dict, key, cell_ID, df, lo_idx, hi_idx):\n",
    "    '''\n",
    "    For a particular cell_ID in parent_dict, generate an array with 5 columns.\n",
    "    \n",
    "    These columns represent:\n",
    "    - Number of cycles remaining until knee onset\n",
    "    - Number of cycles remaining until knee point\n",
    "    - Number of cycles remaining until EOL\n",
    "    - Capacity degradation (Ah) until knee onset\n",
    "    - Capacity degradation (Ah) until knee point\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    parent_dict (type: dict)\n",
    "        Dictionary whose keys are cell IDs, containing 2D cycle/capacity array in a key\n",
    "        \n",
    "    key (type: str)\n",
    "        Dictionary key that is used to identify the cycle/capacity 2D array.\n",
    "        \n",
    "    cell_ID (type: str)\n",
    "        Cell identifier string used to specify the keys of parent_dict to extract cell data.\n",
    "        \n",
    "    df (type: pd.DataFrame)\n",
    "        A DataFrame containing, for each cell, 5 values (cycle number for onset, point, EOL, capacity at onset and point).\n",
    "        This is obtained using the function \"get_knee_and_eol_results\"    \n",
    "    \n",
    "    lo_idx (type: int)\n",
    "        Index in the capacity array that corresponds to the first cycle of time series data.\n",
    "        This is needed if you have taken cycles 2 to 101 for each cell, for example.\n",
    "    \n",
    "    hi_idx (type: int)\n",
    "        Index in the capacity array that corresponds to the last cycle of time series data.\n",
    "        This is needed if you have taken cycles 2 to 101 for each cell, for example.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # Extract the 2D array of cycles/interpolated capacity values from the dictionary\n",
    "    cap_arr = copy.deepcopy(parent_dict[cell_ID][key])\n",
    "    cap_arr = cap_arr[lo_idx:hi_idx+1]\n",
    "\n",
    "    # Create a DataFrame so we can explicitly refer to the column names for assignment\n",
    "    result = pd.DataFrame(np.zeros(shape=(cap_arr.shape[0], 5), dtype=float),\n",
    "                          index=cap_arr[:,0].astype(int),\n",
    "                          columns=['tto', 'ttp', 'tte', 'deg_o', 'deg_p'])\n",
    "\n",
    "    # Populate the result DataFrame with values\n",
    "    result['tto'] = df.at[cell_ID, \"onset\"] - cap_arr[:,0]\n",
    "    result['ttp'] = df.at[cell_ID, \"point\"] - cap_arr[:,0]\n",
    "    result['tte'] = df.at[cell_ID, \"EOL\"] - cap_arr[:,0]\n",
    "    result['deg_o'] = cap_arr[:,1] - df.at[cell_ID, \"onset_y\"]\n",
    "    result['deg_p'] = cap_arr[:,1] - df.at[cell_ID, \"point_y\"]\n",
    "    \n",
    "\n",
    "    # Convert the DataFrame to a numpy array\n",
    "    result_arr = result.to_numpy(copy=True)\n",
    "    \n",
    "    return result_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81078e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the cell IDs\n",
    "cells = list(data.keys())\n",
    "\n",
    "# Instantiate index values to be passed to create_y_target_array.\n",
    "# These are the indices for the capacity arrays for which the cycle\n",
    "# numbers match those whose time series data we are using.\n",
    "lo_idx = 2\n",
    "hi_idx = first_n_cycles\n",
    "\n",
    "# Create the y target array for all cells, for the first N cycles of each cell\n",
    "y_arr = np.vstack([create_y_target_array(parent_dict=data, key='capacity', cell_ID=cell, df=df_out, lo_idx=lo_idx, hi_idx=hi_idx) for cell in cells])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8891ea",
   "metadata": {},
   "source": [
    "### Train, Validation, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "237ebc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the X and y arrays for train, validation and test\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test), cells_dict = get_train_val_test(cell_list=cells,\n",
    "                                                                                      X=X,\n",
    "                                                                                      y=y_arr,\n",
    "                                                                                      index=index,\n",
    "                                                                                      rdm_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ffa025",
   "metadata": {},
   "source": [
    "### Reshape Data for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d22d47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to reshape the data so it's the appropriate shape for 1D CNN / LSTM models.\n",
    "# This will accept arbitrary numbers of features\n",
    "\n",
    "def reshape_for_model(X_arr, to_plot=False):\n",
    "    # Get the shape of the data prior to reshaping for model\n",
    "    features, samples, timesteps = X_arr.shape\n",
    "    \n",
    "    # Instead of creating a new array where we assume a number of features,\n",
    "    # we stack all features present in X_arr.\n",
    "    X_reshaped = np.array([np.vstack(\n",
    "                            [np.vstack(\n",
    "                                [X_arr[j, i, :] for j in range(features)]\n",
    "                            )]).T\n",
    "                           for i in range(samples)])\n",
    "        \n",
    "    if to_plot:\n",
    "        # Plot a random selection of instances to check they look OK\n",
    "        indices = np.random.randint(0, samples, size=25)\n",
    "        fig, ax = plt.subplots(5,5)\n",
    "        for subplot, sample in enumerate(indices):\n",
    "            ax.flatten()[subplot].plot(X_reshaped[sample,:,1])\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    return X_reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88df1efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = reshape_for_model(X_train, to_plot=False)\n",
    "X_val = reshape_for_model(X_val, to_plot=False)\n",
    "X_test = reshape_for_model(X_test, to_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f9dba9",
   "metadata": {},
   "source": [
    "### Per-Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5445399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the scaler_3d function from baumhofer_utils.py to scale the data.\n",
    "# Also convert np.nan values to zeros, if there are np.nan values present in the arrays.\n",
    "X_train_sc, X_val_sc, X_test_sc = scaler_3d(X_train, X_val, X_test, scaler_type=\"minmax\")\n",
    "\n",
    "# Visualise the result\n",
    "plt.plot(X_train_sc[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eee520",
   "metadata": {},
   "source": [
    "### Now we have train, val and test sets for X and y, ready to be used with models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517755fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
